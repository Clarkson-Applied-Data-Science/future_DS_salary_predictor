---
title: "R Notebook"
output: html_notebook
---
```{r echo=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)
library(scales)
library(emmeans)
library(broom) 
library(multcompView)
library(multcomp) 
library(Metrics)
library(caret)
library(FSA)
library(dplyr)
library(ModelMetrics)
library(gbm)
library(DescTools)
library(rstatix)
library(rpart)

``` 

```{r}
salary_data_no_category <- read.csv("Data/datascience.csv")
salary_data_with_category <- read.csv("Data/datascienceold.csv") 
category_counts <- salary_data_with_category %>%
  group_by(job_title, job_category) %>%
  tally() %>%
  arrange(job_title, desc(n)) %>%
  group_by(job_title) %>%
  slice(1) %>%
  ungroup()

salary_df <- salary_data_no_category %>%
  left_join(category_counts %>% dplyr::select(job_title, job_category), by = "job_title") %>%
  mutate(job_category = ifelse(is.na(job_category), "Unknown", job_category)) 

#salary_df$salary_in_usd <- log(salary_df$salary_in_usd)
salary_df <- salary_df %>%
  group_by(job_title) %>%
  filter(n() > 3) %>%
  ungroup()
```

#Removing outliers
```{r}
Q1 <- quantile(salary_df$salary_in_usd, 0.25)
Q3 <- quantile(salary_df$salary_in_usd, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR


salary_df <- salary_df %>%
  filter(salary_in_usd >= lower_bound & salary_in_usd <= upper_bound)

salary_df <- salary_df %>%
  mutate(
    salary_category = case_when(
      salary_in_usd < 110000 ~ "Entry (0-2 years)",
      salary_in_usd >= 110000 & salary_in_usd < 140000 ~ "Mid level (2-5 years)",
      salary_in_usd >= 140000 & salary_in_usd < 180000 ~ "Senior (5-8 years)",
      salary_in_usd >= 180000 & salary_in_usd < 220000 ~ "Staff/Principal (8-10 years)",
      salary_in_usd >= 220000 ~ "Manager (10+ years)",
      TRUE ~ "Uncategorized"
    )
  )
```

# Remove entries that are outside us
```{r}
salary_df <- salary_df %>%
  filter(company_location == "US") 
```

#Checking if the salary is normally distributed to decide to use ANOVA or non parametric
```{r}
set.seed(123) 
sample_salary <- sample(salary_df$salary_in_usd, 5000)

shapiro.test(sample_salary)

salary_df %>%
  group_by(job_title) %>%
   filter(n() >= 3) %>% 
  shapiro_test(salary_in_usd)
```
```{r}
salary_df$salary_in_usd <- as.numeric(salary_df$salary_in_usd)
salary_df$job_title <- as.factor(salary_df$job_title)

top_jobs <- names(sort(table(salary_df$job_title), decreasing = TRUE)[1:10])
filtered_data <- salary_df[salary_df$job_title %in% top_jobs, ]
dunn_result <- dunnTest(salary_in_usd ~ job_title, data = filtered_data, method = "bonferroni")
print(dunn_result) 
```
#ADD THE DATA VISUALIZATION CODE HERE

# Cramer’s V because we have categorical
```{r}
cat_cols <- c("work_year", "experience_level",  "job_title", "employee_residence","remote_ratio", "company_size")

for (i in 1:(length(cat_cols)-1)) {
  for (j in (i+1):length(cat_cols)) {
    v <- CramerV(table(salary_df[[cat_cols[i]]], salary_df[[cat_cols[j]]]))
    cat(paste(cat_cols[i], "vs", cat_cols[j], "→ Cramér's V:", round(v, 3), "\n"))
  }
}
```


# Spliting for training
```{r}
split_index <- createDataPartition(salary_df$job_title, p = 0.7, list = FALSE)

train_data <- salary_df[split_index, ]
test_data <- salary_df[-split_index, ]
```

#Pre-process factoring 
```{r}  
salary_data <- subset(train_data, select = - c(company_location,employee_residence,salary_currency,salary))
test_data <- subset(test_data, select = - c(company_location,employee_residence,salary_currency,salary))

cols_to_factor <- c("work_year", "experience_level", "employment_type", "job_title", "job_category", "remote_ratio", "company_size")
salary_data[cols_to_factor] <- lapply(salary_data[cols_to_factor], as.factor)
test_data[cols_to_factor] <- lapply(test_data[cols_to_factor], as.factor)
```

#Encoding
```{r} 
dummy <- dummyVars(~ work_year + experience_level + remote_ratio + company_size + job_category, data = salary_data)
# Apply one-hot encoding to the dataset
salary_data_one_hot <- predict(dummy, newdata = salary_data)
salary_data_one_hot <- data.frame(salary_data_one_hot)
salary_data_combined <- cbind(salary_data, salary_data_one_hot)
salary_data <- salary_data_combined[, !names(salary_data_combined) %in% c("work_year", "experience_level", "employment_type",  "remote_ratio", "company_size","job_category")]
```

#Scaling
```{r}
min_salary <- min(train_data$salary_in_usd)
max_salary <- max(train_data$salary_in_usd)

salary_data <- salary_data %>%
  mutate(salary_in_usd = (salary_in_usd - min_salary) / (max_salary - min_salary))
#view(salary_data)
```

# SMOTE
```{r}
max_per_title <- max(table(salary_data$job_title))
salary_data <- salary_data %>%
group_by(job_title) %>%
slice(rep(1:n(), length.out = max_per_title)) %>%
ungroup()
```
#Do the model
```{r}
source("./LinearRegression/Model.R")
```

```{r} 
categorical_cols=colnames(salary_df)
categorical_cols
```

# Kruskal-Wallis Test
```{r} 
#categorical_cols=colnames(salary_df)
categorical_cols=c("work_year","experience_level","employment_type","job_title", "employee_residence","remote_ratio","company_size","job_category")
for (col in categorical_cols) {
  formula <- as.formula(paste("salary_in_usd ~", col))
  kw_result <- kruskal.test(formula, data = salary_df)
  cat(col, ": Kruskal-Wallis p-value =", kw_result$p.value, "\n")
}

```

#Do the rest models Here do your test on your own files and share the confusion matrix or we can utlize a central function and call on the files 
```{r}
#source("RandomForest/Model.R")
#source("DecisionTree/Model.R")
#source("./Multinomial/Model.R")

```

#SMOTE BUT NOT USED THIS IS FOR FUTURE USE
```{r}
'''# Identify categorical columns (assuming they are factor or character type)
categorical_vars <- c("work_year", "experience_level", "employment_type", "job_title","salary_currency",
                      "employee_residence","remote_ratio", "company_size","company_location","job_category")

categorical_data <- salary_df[, categorical_vars]

# Determine the maximum count of categories in each categorical feature
max_counts <- sapply(categorical_data, function(col) max(table(col)))

# Apply oversampling to each categorical variable
salary_data_balanced <- salary_df

# Loop through each categorical variable and apply oversampling
for (var in names(categorical_data)) {
  max_per_category <- max(table(salary_df[[var]]))
  salary_data_balanced <- salary_data_balanced %>%
    group_by_at(var) %>%
    slice(rep(1:n(), length.out = max_per_category)) %>%
    ungroup()
}'''
```
